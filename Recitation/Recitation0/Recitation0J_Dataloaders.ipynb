{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-g4uFqlPfW"
      },
      "source": [
        "# Recitation 0J : Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-aBBGabllM9"
      },
      "source": [
        "# Goal\n",
        "Our goal in this recitation is to get comfortable using the dataloader object\n",
        "\n",
        "# Contents\n",
        "\n",
        "\n",
        "1. Introduction to PyTorch DataLoader\n",
        "2. Initializing a DataLoader Object\n",
        "3. Handling Different Batching Strategies\n",
        "4. Customizing Data Loading with Collate Functions\n",
        "5. Leveraging Multi-Process Data Loading\n",
        "6. Optimizing with Pin Memory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRyHXJLC2HdQ"
      },
      "source": [
        "## Manual data feed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WkwH9sfU9RLC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kUBWT2kA-Tm"
      },
      "source": [
        "**1 epoch**: one complete pass of the training dataset through the algorithm\n",
        "\n",
        "**batch_size**: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you will need.\n",
        "\n",
        "\n",
        "**No of iterations = No of batches**: number of passes, each pass using batch_size number of examples.\n",
        "\n",
        "Example: With 100 training examples and batch size of 20 it will take 5 iterations to complete 1 epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "x = a list of 10000 input samples\n",
        "y = a list of 10000 target labels corresponding to x\n",
        "\n",
        "# Load data manually in batches\n",
        "for epoch in range(10):\n",
        "    for i in range(n_batches):\n",
        "        # Local batches and labels\n",
        "        local_X, local_y = x[i*n_batches:(i+1)*n_batches,], y[i*n_batches:(i+1)*n_batches,]\n",
        "\n",
        "        # Your model\n",
        "        [...]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4MVhNRpVScZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFeJx8YzA-Tm"
      },
      "source": [
        "# Dataloaders (PyTorch)\n",
        "\n",
        "Documentation:\n",
        "[Read Docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "\n",
        "The Dataset retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to\n",
        "\n",
        "1.   Pass samples in “minibatches”\n",
        "2.   Reshuffle the data at every epoch to reduce model overfitting\n",
        "3.   Use Python's multiprocessing to speed up data retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RHffau3A-To"
      },
      "source": [
        "# Sample DataLoader\n",
        "\n",
        "Handles data loading logic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataloader will use dataset to create batches, process data etc.\n",
        "# Visit Dataset Recitation for more details\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    # constructor, in this case it contains the data\n",
        "    def __init__(self, xs, ys):\n",
        "        self.input = input\n",
        "        self.target = target\n",
        "\n",
        "    # returns the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    # returns the item at index i\n",
        "    def __getitem__(self, i):\n",
        "        return self.input[i], self.target[i]"
      ],
      "metadata": {
        "id": "ENf4LWb-MZ5P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to train a model to learn that the target = 2 x input, and hence created the following dataset:"
      ],
      "metadata": {
        "id": "uD7Lw07mEbft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating a dummy dataset to test Dataloaders\n",
        "input = list(range(10))\n",
        "target = list(range(0, 20, 2))\n",
        "print('input values: ', input)\n",
        "print('target values: ', target)\n",
        "\n",
        "# Create an instance of MyDataset class\n",
        "dataset = MyDataset(input, target)\n",
        "print(\"The second sample is: \", dataset[2]) # returns the tuple (input[2], target[2])\n",
        "# This is basically same as\n",
        "print(\"The second sample is: \", dataset.__getitem__(2))\n",
        "# Which the dataloader needs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6rOUn8rM53n",
        "outputId": "41810184-fa96-4b45-b097-91db1edebdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input values:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "target values:  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "The second sample is:  (2, 4)\n",
            "The second sample is:  (2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look at different ways of creating the Dataloader object using the Dataloader class\n"
      ],
      "metadata": {
        "id": "J1fO482wRkAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q_QDR9FA-To",
        "outputId": "ca9c3c28-759d-4977-d434-d1671e5522b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([0]), batch of labels: tensor([0])\n",
            "batch of inputs: tensor([1]), batch of labels: tensor([2])\n",
            "batch of inputs: tensor([2]), batch of labels: tensor([4])\n",
            "batch of inputs: tensor([3]), batch of labels: tensor([6])\n",
            "batch of inputs: tensor([4]), batch of labels: tensor([8])\n",
            "batch of inputs: tensor([5]), batch of labels: tensor([10])\n",
            "batch of inputs: tensor([6]), batch of labels: tensor([12])\n",
            "batch of inputs: tensor([7]), batch of labels: tensor([14])\n",
            "batch of inputs: tensor([8]), batch of labels: tensor([16])\n",
            "batch of inputs: tensor([9]), batch of labels: tensor([18])\n"
          ]
        }
      ],
      "source": [
        "# batch size of 1, so we the size of x and y is 1 and no shuffling\n",
        "for x, y in DataLoader(dataset):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size of 4, so x and y both have a size of 4, no shuffling\n",
        "for x, y in DataLoader(dataset, batch_size=4):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klMIFFxPR7qY",
        "outputId": "4a5955f3-6e50-4754-9546-14e381fc1da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([0, 1, 2, 3]), batch of labels: tensor([0, 2, 4, 6])\n",
            "batch of inputs: tensor([4, 5, 6, 7]), batch of labels: tensor([ 8, 10, 12, 14])\n",
            "batch of inputs: tensor([8, 9]), batch of labels: tensor([16, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHzO_2gLA-To",
        "outputId": "f583efd6-9b0e-4080-dd51-54eafa89dacc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([4, 9, 8, 5]), batch of labels: tensor([ 8, 18, 16, 10])\n",
            "batch of inputs: tensor([0, 1, 7, 6]), batch of labels: tensor([ 0,  2, 14, 12])\n",
            "batch of inputs: tensor([2, 3]), batch of labels: tensor([4, 6])\n"
          ]
        }
      ],
      "source": [
        "# batch size of 4, so x and y both have a size of 4, random shuffle\n",
        "for x, y in DataLoader(dataset, batch_size=4, shuffle=True):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size of 4, drop the last batch with less than 4 samples\n",
        "for x, y in DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsk2wl9QZ0e",
        "outputId": "02f8bf1d-ac5b-4a3c-e235-239d3235104c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([4, 9, 0, 6]), batch of labels: tensor([ 8, 18,  0, 12])\n",
            "batch of inputs: tensor([3, 8, 5, 7]), batch of labels: tensor([ 6, 16, 10, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJUw-SARA-Tp"
      },
      "source": [
        "# Collate function\n",
        "\n",
        "A dataloader parameter which can be customized to achieve custom automatic batching.\n",
        "\n",
        "You may apply some transformation in the collate function;\n",
        "One can choose to apply transformation in the collate function instaed of dataset class if transformation needs to be applied on batches.\n",
        "Also, since data loader support multiprocess through multi-workers, hence ```collate_fn()``` also can take advantage of multi-workers performance speed up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqd2U3DcA-Tp",
        "outputId": "ea18a424-4ce7-453a-a90e-3224e7029b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input values:  [0 1 2 3 4 5 6 7 8 9]\n",
            "target values:  [ 0  2  4  6  8 10 12 14 16 18]\n",
            "batch of inputs: [-0.62092042 -0.23284516  1.31945589  0.93138063 -1.39707095], batch of labels: (8, 10, 18, 16, 4)\n",
            "batch of inputs: [-0.87988269  0.95320625  1.31982403 -0.14664711 -1.24650048], batch of labels: (2, 12, 14, 6, 0)\n"
          ]
        }
      ],
      "source": [
        "# Create an object of the custom dataset class\n",
        "class MyNormalDataset(Dataset):\n",
        "    # constructor, in this case it contains the data\n",
        "    def __init__(self, xs, ys):\n",
        "        self.input = input\n",
        "        self.target = target\n",
        "\n",
        "    # returns the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    # returns the item at index i\n",
        "    def __getitem__(self, i):\n",
        "        return self.input[i], self.target[i]# create a dict of arguments, another way of passing arguments\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        x, y = zip(*batch)\n",
        "        x_mean = np.mean(x)\n",
        "        x_std = np.std(x)\n",
        "        x_normal = (x-x_mean)/(x_std+1e-9)\n",
        "        return x_normal, y\n",
        "\n",
        "\n",
        "input = np.array(list(range(10)))\n",
        "target = np.array(list(range(0, 20, 2)))\n",
        "print('input values: ', input)\n",
        "print('target values: ', target)\n",
        "\n",
        "# Create an instance of MyDataset class\n",
        "dataset = MyNormalDataset(input, target)\n",
        "# Use the custom collate_fn\n",
        "# pass the arguments\n",
        "train_dataloader_custom = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn)\n",
        "\n",
        "# Display collated inputs and labels.\n",
        "for i, (x, y) in enumerate(train_dataloader_custom):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single and multi-process loading"
      ],
      "metadata": {
        "id": "_miJSuhbX4rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the ```num_workers``` to specify how many subprocesses to use for data loading. \\\n",
        "0 means that the data will be loaded in the main process. \\"
      ],
      "metadata": {
        "id": "Y04JQml3YPJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 subprocesses\n",
        "train_dataloader_fast = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=2)"
      ],
      "metadata": {
        "id": "Kg8Wc1HSX8cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum subprocesses you can use depends on the machine you are training on\n",
        "# you can try to increase it until you see a warning.\n",
        "\n",
        "train_dataloader_fast = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=4)"
      ],
      "metadata": {
        "id": "nEvm1fHvaGC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use ```pin_memory``` to copy Tensors into device/CUDA pinned memory before returning them -> faster processing."
      ],
      "metadata": {
        "id": "--nS-mwFYnUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_faster = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=4, pin_memory= True)"
      ],
      "metadata": {
        "id": "EHIMnMhuYnkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}